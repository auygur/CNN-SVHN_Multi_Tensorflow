{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Multi-Digit traning on SVHN dataset\n",
    "=============\n",
    "\n",
    "we build a convolutional neural network to train a model to read the number up to 5 digit form the stree-view-house-number (SVHN).\n",
    "\n",
    "- Arman Uygur # au2205\n",
    "- Jonathan Galsurkar #jfg2150\n",
    "- Nitesh Surtani #ns3148\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Importing Packages\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import time\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing the pre-processed SVHN dataset, it is currently stored as H5 format.\n",
    "\"\"\"\n",
    "# Opening the h5 file\n",
    "h5 = h5py.File('SVHN_preprocessed.h5','r')\n",
    "\n",
    "# Extract the datasets\n",
    "train_dataset = h5['train_dataset'][:]\n",
    "train_labels = h5['train_labels'][:]\n",
    "test_dataset = h5['test_dataset'][:]\n",
    "test_labels = h5['test_labels'][:]\n",
    "valid_dataset = h5['valid_dataset'][:]\n",
    "valid_labels = h5['valid_labels'][:]\n",
    "\n",
    "# Closing the h5 file\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_labels = 11\n",
    "num_channels = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Defining some helper functions.\n",
    "- accuracy function returns a prediction accuracy\n",
    "- get_conv_params function returns a tf variable of a given kernel shape, input and output numbers of a conv layer.\n",
    "- get_fc_params functions returns a tf variable of given input and output numbers of a fully connected layer\n",
    "\"\"\"\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])\n",
    "\n",
    "def get_conv_params(kernel_shape, in_channel, out_channel):\n",
    "    return tf.Variable(tf.truncated_normal([kernel_shape, kernel_shape, in_channel, out_channel], stddev=0.1)), tf.Variable(tf.zeros([out_channel]))\n",
    "\n",
    "def get_fc_params(in_channel, out_channel):\n",
    "    return tf.Variable(tf.truncated_normal([in_channel, out_channel], stddev=0.1)), tf.Variable(tf.zeros([out_channel]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Defining layer functions.\n",
    "- conv_layer functions takes following parameters:\n",
    "    -input\n",
    "    -weight\n",
    "    -bias\n",
    "    -stride : if none, defaults to 1 stride, this is a number variable to specify how many strides should be applied\n",
    "    -pooling: this is a boolean variable, if True, it applies a 2x2 max pooling\n",
    "    -activation: this variable is by default relu, but it could also be a maxout function\n",
    "    -batch_norm: this is a boolean variable, if True, it applies a batch normalization.\n",
    "    \n",
    "- fc_layer function takes following parameters:\n",
    "    -input\n",
    "    -weight\n",
    "    -bias\n",
    "    -activation: this is a boolean variable, if True, it applies a relu activation.\n",
    "\"\"\"\n",
    "\n",
    "def conv_layer(input_x, weight, bias, stride = None, pooling = False, activation = tf.nn.relu, batch_norm = True):\n",
    "    conv_out = tf.add(tf.nn.conv2d(input_x, weight, [1, 1, 1, 1], padding='SAME'), bias)\n",
    "    \n",
    "    if batch_norm:\n",
    "        conv_out = tf.layers.batch_normalization(conv_out, training = True)\n",
    "    if activation == 'maxout':\n",
    "        cell_out = tf.contrib.layers.maxout(conv_out, num_units=48)\n",
    "    else:\n",
    "        cell_out = activation(conv_out)\n",
    "    if pooling:\n",
    "        cell_out = tf.layers.max_pooling2d(inputs=cell_out, pool_size=[2, 2], strides=stride, padding='SAME')\n",
    "    return cell_out\n",
    "\n",
    "def fc_layer(input_x, weight, bias, activation = False):\n",
    "    cell_out = tf.add(tf.matmul(input_x, weight), bias)\n",
    "    if activation:\n",
    "        cell_out = tf.nn.relu(cell_out)\n",
    "    return cell_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Defining model function. It follows the following architecture:\n",
    "\n",
    "Input -> Conv Layer 1 -> Conv Layer 2 -> Conv Layer 3 -> Conv Layer 4 -> Conv Layer 5 -> Conv Layer 6 -> Conv Layer 7 \n",
    "-> Conv Layer 8 -> Flatten Output -> Fully Connected Layer 1 -> Fully Connected Layer 2 -> 5 Softmax Logits -> Output\n",
    "\n",
    "Each convulutional layer includes a dropout (0.9 in best model), a 2x2 max pooling, stride 2 and 1 (alternates), and a relu\n",
    "activation function (except the first one, it has maxout function)\n",
    "\n",
    "In different scenarios, we also tried different dropout probs (0.75 for Convs and 0.5 for fc layers), 9 Conv layers, 5 Conv \n",
    "layers, different learning rates (1e-2 vs 1e-3), different sizes of fc units (256 vs 3072)\n",
    "\"\"\"\n",
    "\n",
    "def model(data, keepProb = 0.90):\n",
    "    conv1 = tf.nn.dropout(conv_layer(data, w1, b1, pooling = True, stride=2, activation='maxout'), keepProb)\n",
    "    conv2 = tf.nn.dropout(conv_layer(conv1, w2, b2, pooling = True, stride=1), keepProb)\n",
    "    conv3 = tf.nn.dropout(conv_layer(conv2, w3, b3, pooling = True, stride=2), keepProb)\n",
    "    conv4 = tf.nn.dropout(conv_layer(conv3, w4, b4, pooling = True, stride=1), keepProb)\n",
    "    conv5 = tf.nn.dropout(conv_layer(conv4, w5, b5, pooling = True, stride=2), keepProb)\n",
    "    conv6 = tf.nn.dropout(conv_layer(conv5, w6, b6, pooling = True, stride=1), keepProb)\n",
    "    conv7 = tf.nn.dropout(conv_layer(conv6, w7, b7, pooling = True, stride=2), keepProb)\n",
    "    conv8 = tf.nn.dropout(conv_layer(conv7, w8, b8, pooling = True, stride=1), keepProb)\n",
    "#     conv9 = tf.nn.dropout(conv_layer(conv8, w11, b11, pooling = True, stride=2), keepProb)\n",
    "\n",
    "    #full-connected layers - 2 layers -     \n",
    "    shape = conv8.get_shape().as_list()\n",
    "    reshape = tf.reshape(conv8, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    fc1 = tf.nn.dropout(fc_layer(reshape, w9, b9), keepProb)\n",
    "    fc2 = tf.nn.dropout(fc_layer(fc1, w10, b10), keepProb)\n",
    "    \n",
    "    logitsC1 = fc_layer(fc2, w11, b11)\n",
    "    logitsC2 = fc_layer(fc2, w12, b12)\n",
    "    logitsC3 = fc_layer(fc2, w13, b13)\n",
    "    logitsC4 = fc_layer(fc2, w14, b14)\n",
    "    logitsC5 = fc_layer(fc2, w15, b15)\n",
    "    logits = tf.stack([logitsC1, logitsC2, logitsC3, logitsC4, logitsC5])\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining variables and predicting. Using 3x3 kernel for first conv layer, and 5x5 for the rest. Both FC Layers are getting 3072\n",
    "units each. Since we used 100 GB RAM and 2 GPU, we could use batch_size = 1024. We used the same units mentioned in paper for \n",
    "each Conv layer - specifically:\n",
    "Conv Layer 1:  48 Units\n",
    "Conv Layer 2:  64 Units\n",
    "Conv Layer 3:  128 Units\n",
    "Conv Layer 4:  160 Units\n",
    "Conv Layer 5:  192 Units\n",
    "Conv Layer 6:  192 Units\n",
    "Conv Layer 7:  192 Units\n",
    "Conv Layer 8:  192 Units\n",
    "\n",
    "FC Layer1: 192 x 4 (4 multiplier depends on the Conv Architecture) and 3072 units\n",
    "FC Layer2: 3072 units\n",
    "\n",
    "\"\"\"\n",
    "batch_size = 512\n",
    "kernel1 = 5\n",
    "kernel2 = 3\n",
    "\n",
    "num_hidden = 256\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data\n",
    "    X_train = tf.placeholder(tf.float32, shape=(batch_size, train_dataset.shape[1], train_dataset.shape[2], num_channels))\n",
    "    y_train = tf.placeholder(tf.int32, shape=(batch_size, 5))\n",
    "    X_val = tf.constant(valid_dataset)\n",
    "    X_test = tf.constant(test_dataset)\n",
    "   \n",
    "    # Variables\n",
    "    w1, b1 = get_conv_params(kernel2, num_channels, 48)\n",
    "    w2, b2 = get_conv_params(kernel1, 48, 64)\n",
    "    w3, b3 = get_conv_params(kernel1, 64, 128)\n",
    "    w4, b4 = get_conv_params(kernel1, 128, 160)\n",
    "    w5, b5 = get_conv_params(kernel1, 160, 192)\n",
    "    w6, b6 = get_conv_params(kernel1, 192, 192)\n",
    "    w7, b7 = get_conv_params(kernel1, 192, 192)\n",
    "    w8, b8 = get_conv_params(kernel1, 192, 192)\n",
    "    \n",
    "    \n",
    "    #full-connected layers - 2 layers - \n",
    "    w9, b9 = get_fc_params(192*4, num_hidden)\n",
    "    w10, b10 = get_fc_params(num_hidden, num_hidden)\n",
    "\n",
    "    # parameters for logits\n",
    "    w11, b11 = get_fc_params(num_hidden, num_labels)\n",
    "    w12, b12 = get_fc_params(num_hidden, num_labels)\n",
    "    w13, b13 = get_fc_params(num_hidden, num_labels)\n",
    "    w14, b14 = get_fc_params(num_hidden, num_labels)\n",
    "    w15, b15 = get_fc_params(num_hidden, num_labels)\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(X_train)\n",
    "    loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_train[:,0], logits=logits[0]))\n",
    "    loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_train[:,1], logits=logits[1]))\n",
    "    loss3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_train[:,2], logits=logits[2]))\n",
    "    loss4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_train[:,3], logits=logits[3]))\n",
    "    loss5 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_train[:,4], logits=logits[4])) \n",
    "    loss = loss1+loss2+loss3+loss4+loss5\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_pred = tf.nn.softmax(logits)\n",
    "    valid_pred = tf.nn.softmax(model(X_val, 1.0))\n",
    "    test_pred = tf.nn.softmax(model(X_test, 1.0))\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy at step 0: 13.4% | Validation accuracy: 56.4% | Batch loss: 35.25 \n",
      "Batch accuracy at step 100: 53.5% | Validation accuracy: 58.1% | Batch loss: 7.31 \n",
      "Batch accuracy at step 200: 55.8% | Validation accuracy: 59.7% | Batch loss: 6.44 \n",
      "Batch accuracy at step 300: 57.7% | Validation accuracy: 60.2% | Batch loss: 6.20 \n",
      "Batch accuracy at step 400: 59.8% | Validation accuracy: 63.1% | Batch loss: 5.85 \n",
      "Batch accuracy at step 500: 70.5% | Validation accuracy: 69.3% | Batch loss: 4.40 \n",
      "Batch accuracy at step 600: 73.5% | Validation accuracy: 75.9% | Batch loss: 3.88 \n",
      "Batch accuracy at step 700: 81.0% | Validation accuracy: 83.1% | Batch loss: 2.71 \n",
      "Batch accuracy at step 800: 85.3% | Validation accuracy: 85.6% | Batch loss: 2.11 \n",
      "Batch accuracy at step 900: 82.1% | Validation accuracy: 89.5% | Batch loss: 3.07 \n",
      "Batch accuracy at step 1000: 90.4% | Validation accuracy: 90.0% | Batch loss: 1.57 \n",
      "Batch accuracy at step 1100: 91.4% | Validation accuracy: 90.6% | Batch loss: 1.57 \n",
      "Batch accuracy at step 1200: 92.6% | Validation accuracy: 92.2% | Batch loss: 1.16 \n",
      "Batch accuracy at step 1300: 93.1% | Validation accuracy: 92.3% | Batch loss: 1.07 \n",
      "Batch accuracy at step 1400: 92.5% | Validation accuracy: 93.4% | Batch loss: 1.23 \n",
      "Batch accuracy at step 1500: 94.4% | Validation accuracy: 93.2% | Batch loss: 0.92 \n",
      "Batch accuracy at step 1600: 94.6% | Validation accuracy: 93.5% | Batch loss: 0.76 \n",
      "Batch accuracy at step 1700: 94.6% | Validation accuracy: 93.4% | Batch loss: 0.79 \n",
      "Batch accuracy at step 1800: 92.0% | Validation accuracy: 94.5% | Batch loss: 1.40 \n",
      "Batch accuracy at step 1900: 95.5% | Validation accuracy: 94.1% | Batch loss: 0.80 \n",
      "Batch accuracy at step 2000: 96.0% | Validation accuracy: 93.8% | Batch loss: 0.72 \n",
      "Batch accuracy at step 2100: 96.2% | Validation accuracy: 94.1% | Batch loss: 0.59 \n",
      "Batch accuracy at step 2200: 96.5% | Validation accuracy: 94.1% | Batch loss: 0.56 \n",
      "Batch accuracy at step 2300: 95.0% | Validation accuracy: 94.9% | Batch loss: 0.83 \n",
      "Batch accuracy at step 2400: 96.1% | Validation accuracy: 94.4% | Batch loss: 0.60 \n",
      "Batch accuracy at step 2500: 96.6% | Validation accuracy: 94.6% | Batch loss: 0.56 \n",
      "Batch accuracy at step 2600: 97.3% | Validation accuracy: 94.5% | Batch loss: 0.48 \n",
      "Batch accuracy at step 2700: 93.7% | Validation accuracy: 94.9% | Batch loss: 1.02 \n",
      "Batch accuracy at step 2800: 97.1% | Validation accuracy: 94.7% | Batch loss: 0.51 \n",
      "Batch accuracy at step 2900: 97.5% | Validation accuracy: 94.3% | Batch loss: 0.43 \n",
      "Batch accuracy at step 3000: 96.7% | Validation accuracy: 94.2% | Batch loss: 0.63 \n",
      "Batch accuracy at step 3100: 97.8% | Validation accuracy: 94.4% | Batch loss: 0.38 \n",
      "Batch accuracy at step 3200: 95.4% | Validation accuracy: 94.9% | Batch loss: 0.79 \n",
      "Batch accuracy at step 3300: 97.6% | Validation accuracy: 94.7% | Batch loss: 0.49 \n",
      "Batch accuracy at step 3400: 98.3% | Validation accuracy: 94.9% | Batch loss: 0.31 \n",
      "Batch accuracy at step 3500: 98.0% | Validation accuracy: 94.5% | Batch loss: 0.34 \n",
      "Batch accuracy at step 3600: 95.5% | Validation accuracy: 95.1% | Batch loss: 0.82 \n",
      "Batch accuracy at step 3700: 97.7% | Validation accuracy: 94.7% | Batch loss: 0.37 \n",
      "Batch accuracy at step 3800: 98.0% | Validation accuracy: 94.5% | Batch loss: 0.37 \n",
      "Batch accuracy at step 3900: 97.9% | Validation accuracy: 94.8% | Batch loss: 0.33 \n",
      "Batch accuracy at step 4000: 97.3% | Validation accuracy: 94.0% | Batch loss: 0.45 \n",
      "Batch accuracy at step 4100: 95.7% | Validation accuracy: 95.0% | Batch loss: 0.70 \n",
      "Batch accuracy at step 4200: 98.2% | Validation accuracy: 94.9% | Batch loss: 0.34 \n",
      "Batch accuracy at step 4300: 98.2% | Validation accuracy: 94.7% | Batch loss: 0.27 \n",
      "Batch accuracy at step 4400: 98.1% | Validation accuracy: 94.9% | Batch loss: 0.28 \n",
      "Batch accuracy at step 4500: 96.4% | Validation accuracy: 94.5% | Batch loss: 0.64 \n",
      "Batch accuracy at step 4600: 98.7% | Validation accuracy: 94.7% | Batch loss: 0.25 \n",
      "Batch accuracy at step 4700: 98.6% | Validation accuracy: 94.6% | Batch loss: 0.24 \n",
      "Batch accuracy at step 4800: 98.9% | Validation accuracy: 94.8% | Batch loss: 0.21 \n",
      "Batch accuracy at step 4900: 98.4% | Validation accuracy: 94.1% | Batch loss: 0.31 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simulation Step: Ran all scearios up to 5,000 iterations.\n",
    "\"\"\"\n",
    "num_iter = 5000\n",
    "best_acc = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "   \n",
    "    for step in range(num_iter):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_x = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_y = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {X_train : batch_x, y_train : batch_y}\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, loss, train_pred], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 100 == 0):\n",
    "            train_acc = accuracy(predictions, batch_y[:,:])\n",
    "            val_acc = accuracy(valid_pred.eval(), valid_labels[:,:])\n",
    "            print('Batch accuracy at step {0}: {1:.1f}% | Validation accuracy: {2:.1f}% | Batch loss: {3:.2f} '\n",
    "                  .format(step, train_acc, val_acc,l))\n",
    "\n",
    "            # saving the best model (one with highest validation accuracy)\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                saver.save(session, './best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model\n",
      "Test accuracy 95.59%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Restoring the best model (one with highest validation accuracy) to predict test labels.\n",
    "\"\"\"\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, \"./best_model\")\n",
    "    prediction = session.run(test_pred, feed_dict={X_test : test_dataset,})\n",
    "    test_acc = accuracy(prediction, test_labels[:,:])\n",
    "print('Test accuracy {0:.2f}%'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy at step 0: 6.2% | Validation accuracy: 54.7% | Batch loss: 67.94 \n",
      "Batch accuracy at step 100: 53.3% | Validation accuracy: 58.1% | Batch loss: 9.01 \n",
      "Batch accuracy at step 200: 55.1% | Validation accuracy: 60.6% | Batch loss: 7.89 \n",
      "Batch accuracy at step 300: 60.8% | Validation accuracy: 66.2% | Batch loss: 6.64 \n",
      "Batch accuracy at step 400: 68.8% | Validation accuracy: 72.1% | Batch loss: 5.20 \n",
      "Batch accuracy at step 500: 78.4% | Validation accuracy: 79.3% | Batch loss: 3.82 \n",
      "Batch accuracy at step 600: 80.2% | Validation accuracy: 82.6% | Batch loss: 3.26 \n",
      "Batch accuracy at step 700: 83.7% | Validation accuracy: 86.0% | Batch loss: 2.69 \n",
      "Batch accuracy at step 800: 86.7% | Validation accuracy: 87.6% | Batch loss: 2.21 \n",
      "Batch accuracy at step 900: 82.9% | Validation accuracy: 89.5% | Batch loss: 2.89 \n",
      "Batch accuracy at step 1000: 89.3% | Validation accuracy: 90.4% | Batch loss: 1.71 \n",
      "Batch accuracy at step 1100: 90.0% | Validation accuracy: 91.0% | Batch loss: 1.80 \n",
      "Batch accuracy at step 1200: 91.4% | Validation accuracy: 91.3% | Batch loss: 1.41 \n",
      "Batch accuracy at step 1300: 92.1% | Validation accuracy: 92.0% | Batch loss: 1.28 \n",
      "Batch accuracy at step 1400: 90.9% | Validation accuracy: 92.0% | Batch loss: 1.37 \n",
      "Batch accuracy at step 1500: 93.5% | Validation accuracy: 92.4% | Batch loss: 1.10 \n",
      "Batch accuracy at step 1600: 93.6% | Validation accuracy: 93.1% | Batch loss: 0.96 \n",
      "Batch accuracy at step 1700: 93.8% | Validation accuracy: 92.9% | Batch loss: 0.93 \n",
      "Batch accuracy at step 1800: 90.8% | Validation accuracy: 93.7% | Batch loss: 1.60 \n",
      "Batch accuracy at step 1900: 94.4% | Validation accuracy: 93.5% | Batch loss: 0.95 \n",
      "Batch accuracy at step 2000: 95.0% | Validation accuracy: 93.7% | Batch loss: 0.92 \n",
      "Batch accuracy at step 2100: 95.2% | Validation accuracy: 93.6% | Batch loss: 0.77 \n",
      "Batch accuracy at step 2200: 95.4% | Validation accuracy: 94.0% | Batch loss: 0.75 \n",
      "Batch accuracy at step 2300: 93.8% | Validation accuracy: 93.7% | Batch loss: 1.01 \n",
      "Batch accuracy at step 2400: 96.2% | Validation accuracy: 93.8% | Batch loss: 0.64 \n",
      "Batch accuracy at step 2500: 96.2% | Validation accuracy: 94.3% | Batch loss: 0.60 \n",
      "Batch accuracy at step 2600: 95.9% | Validation accuracy: 94.3% | Batch loss: 0.67 \n",
      "Batch accuracy at step 2700: 93.0% | Validation accuracy: 94.2% | Batch loss: 1.11 \n",
      "Batch accuracy at step 2800: 96.0% | Validation accuracy: 94.2% | Batch loss: 0.68 \n",
      "Batch accuracy at step 2900: 96.9% | Validation accuracy: 94.5% | Batch loss: 0.60 \n",
      "Batch accuracy at step 3000: 96.0% | Validation accuracy: 94.2% | Batch loss: 0.73 \n",
      "Batch accuracy at step 3100: 97.1% | Validation accuracy: 94.4% | Batch loss: 0.47 \n",
      "Batch accuracy at step 3200: 95.4% | Validation accuracy: 94.2% | Batch loss: 0.78 \n",
      "Batch accuracy at step 3300: 97.1% | Validation accuracy: 94.1% | Batch loss: 0.54 \n",
      "Batch accuracy at step 3400: 97.6% | Validation accuracy: 94.5% | Batch loss: 0.41 \n",
      "Batch accuracy at step 3500: 97.1% | Validation accuracy: 94.3% | Batch loss: 0.46 \n",
      "Batch accuracy at step 3600: 95.3% | Validation accuracy: 94.1% | Batch loss: 0.81 \n",
      "Batch accuracy at step 3700: 97.1% | Validation accuracy: 94.7% | Batch loss: 0.46 \n",
      "Batch accuracy at step 3800: 97.2% | Validation accuracy: 94.3% | Batch loss: 0.45 \n",
      "Batch accuracy at step 3900: 97.4% | Validation accuracy: 94.1% | Batch loss: 0.41 \n",
      "Batch accuracy at step 4000: 97.4% | Validation accuracy: 94.5% | Batch loss: 0.52 \n",
      "Batch accuracy at step 4100: 95.7% | Validation accuracy: 94.2% | Batch loss: 0.70 \n",
      "Batch accuracy at step 4200: 98.0% | Validation accuracy: 93.9% | Batch loss: 0.44 \n",
      "Batch accuracy at step 4300: 98.0% | Validation accuracy: 94.8% | Batch loss: 0.36 \n",
      "Batch accuracy at step 4400: 97.7% | Validation accuracy: 94.4% | Batch loss: 0.36 \n",
      "Batch accuracy at step 4500: 95.4% | Validation accuracy: 93.2% | Batch loss: 0.81 \n",
      "Batch accuracy at step 4600: 98.1% | Validation accuracy: 93.9% | Batch loss: 0.31 \n",
      "Batch accuracy at step 4700: 97.9% | Validation accuracy: 93.9% | Batch loss: 0.32 \n",
      "Batch accuracy at step 4800: 98.3% | Validation accuracy: 93.7% | Batch loss: 0.28 \n",
      "Batch accuracy at step 4900: 98.5% | Validation accuracy: 93.4% | Batch loss: 0.31 \n",
      "Training Time: 1299.97 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simulation Example:\n",
    "5 Conv Layer, 1 FC Layer with FC units = 256 performance\n",
    "\n",
    "Defining variables and predicting. Using 3x3 kernel for first conv layer, and 5x5 for the rest. Both FC Layers are getting 3072\n",
    "units each. Since we used 100 GB RAM and 2 GPU, we could use batch_size = 1024. We used the same units mentioned in paper for \n",
    "each Conv layer - specifically:\n",
    "Conv Layer 1:  48 Units\n",
    "Conv Layer 2:  64 Units\n",
    "Conv Layer 3:  128 Units\n",
    "Conv Layer 4:  160 Units\n",
    "Conv Layer 5:  192 Units\n",
    "\n",
    "FC Layer1: 192 x 4 (4 multiplier depends on the Conv Architecture) and 256 units\n",
    "\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "def small_model(data, keepProb = 0.9):\n",
    "    conv1 = tf.nn.dropout(conv_layer(data, w1, b1, pooling = True, stride=2, activation='maxout'), keepProb)\n",
    "    conv2 = tf.nn.dropout(conv_layer(conv1, w2, b2, pooling = True, stride=1), keepProb)\n",
    "    conv3 = tf.nn.dropout(conv_layer(conv2, w3, b3, pooling = True, stride=2), keepProb)\n",
    "    conv4 = tf.nn.dropout(conv_layer(conv3, w4, b4, pooling = True, stride=1), keepProb)\n",
    "    conv5 = tf.nn.dropout(conv_layer(conv4, w5, b5, pooling = True, stride=2), keepProb)\n",
    "\n",
    "    #full-connected layers - 2 layers -     \n",
    "    shape = conv5.get_shape().as_list()\n",
    "    reshape = tf.reshape(conv5, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    fc1 = tf.nn.dropout(fc_layer(reshape, w9, b9), keepProb)\n",
    "    \n",
    "    logitsC1 = fc_layer(fc1, w11, b11)\n",
    "    logitsC2 = fc_layer(fc1, w12, b12)\n",
    "    logitsC3 = fc_layer(fc1, w13, b13)\n",
    "    logitsC4 = fc_layer(fc1, w14, b14)\n",
    "    logitsC5 = fc_layer(fc1, w15, b15)\n",
    "    \n",
    "    logits = tf.stack([logitsC1, logitsC2, logitsC3, logitsC4, logitsC5])\n",
    "    return logits\n",
    "\n",
    "batch_size = 512\n",
    "kernel1 = 5\n",
    "kernel2 = 3\n",
    "\n",
    "num_hidden = 256\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data\n",
    "    X_train = tf.placeholder(tf.float32, shape=(batch_size, train_dataset.shape[1], train_dataset.shape[2], num_channels))\n",
    "    y_train = tf.placeholder(tf.int32, shape=(batch_size, 5))\n",
    "    X_val = tf.constant(valid_dataset)\n",
    "    X_test = tf.constant(test_dataset)\n",
    "   \n",
    "    # Variables\n",
    "    w1, b1 = get_conv_params(kernel2, num_channels, 48)\n",
    "    w2, b2 = get_conv_params(kernel1, 48, 64)\n",
    "    w3, b3 = get_conv_params(kernel1, 64, 128)\n",
    "    w4, b4 = get_conv_params(kernel1, 128, 160)\n",
    "    w5, b5 = get_conv_params(kernel1, 160, 192)\n",
    "    \n",
    "    \n",
    "    #full-connected layers - 2 layers - \n",
    "    w9, b9 = get_fc_params(192*4*4, num_hidden)\n",
    "\n",
    "    # parameters for logits\n",
    "    w11, b11 = get_fc_params(num_hidden, num_labels)\n",
    "    w12, b12 = get_fc_params(num_hidden, num_labels)\n",
    "    w13, b13 = get_fc_params(num_hidden, num_labels)\n",
    "    w14, b14 = get_fc_params(num_hidden, num_labels)\n",
    "    w15, b15 = get_fc_params(num_hidden, num_labels)\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = small_model(X_train)\n",
    "    loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_train[:,0], logits=logits[0]))\n",
    "    loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_train[:,1], logits=logits[1]))\n",
    "    loss3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_train[:,2], logits=logits[2]))\n",
    "    loss4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_train[:,3], logits=logits[3]))\n",
    "    loss5 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_train[:,4], logits=logits[4])) \n",
    "    loss = loss1+loss2+loss3+loss4+loss5\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_pred = tf.nn.softmax(logits)\n",
    "    valid_pred = tf.nn.softmax(small_model(X_val, 1.0))\n",
    "    test_pred = tf.nn.softmax(small_model(X_test, 1.0))\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "num_iter = 5000\n",
    "best_acc = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "   \n",
    "    for step in range(num_iter):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_x = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_y = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {X_train : batch_x, y_train : batch_y}\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, loss, train_pred], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 100 == 0):\n",
    "            train_acc = accuracy(predictions, batch_y[:,:])\n",
    "            val_acc = accuracy(valid_pred.eval(), valid_labels[:,:])\n",
    "            print('Batch accuracy at step {0}: {1:.1f}% | Validation accuracy: {2:.1f}% | Batch loss: {3:.2f} '\n",
    "                  .format(step, train_acc, val_acc,l))\n",
    "\n",
    "            # saving the best model (one with highest validation accuracy)\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                saver.save(session, './best_model_5ConvModel')\n",
    "\n",
    "t1 = time.time()\n",
    "print('Training Time: {0:.2f} seconds'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model_5ConvModel\n",
      "Test accuracy 94.04%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Restoring the best small architecture model (one with highest validation accuracy) to predict test labels.\n",
    "\"\"\"\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, \"./best_model_5ConvModel\")\n",
    "    prediction = session.run(test_pred, feed_dict={X_test : test_dataset,})\n",
    "    test_acc = accuracy(prediction, test_labels[:,:])\n",
    "print('Test accuracy {0:.2f}%'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
